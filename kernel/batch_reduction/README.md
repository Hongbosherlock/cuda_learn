# Batch Reduction (Row-wise Sum) - CUDA Implementation

æŒ‰è¡Œæ±‚å’Œçš„ CUDA å®ç°ï¼Œæ¯è¡Œç”±ä¸€ä¸ª block ç‹¬ç«‹å¤„ç†ã€‚

## ğŸ¯ å®ç°æ€è·¯

### æ ¸å¿ƒè®¾è®¡
- **Grid Size = m**ï¼šæ¯ä¸ª block å¤„ç†ä¸€è¡Œ
- **Block Size = 256**ï¼šæ¯ä¸ª block å†…çš„çº¿ç¨‹æ•°ï¼ˆå¯è°ƒï¼‰
- **æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ **ï¼šä½¿ç”¨æ­¥é•¿å¾ªç¯

```
input:  [m, n]  (m rows, n columns)
         â†“
      m blocks (grid_size = m)
         â†“
output: [m]     (m row sums)
```

## ğŸ“‹ ä¸¤ç§å®ç°ç‰ˆæœ¬

### 1. Optimized Versionï¼ˆæ¨èï¼‰

ä½¿ç”¨ **Warp Shuffle** ä¼˜åŒ–ï¼š

```cpp
__global__ void reduce_optimized(input, output, n) {
    // Step 1: æ¯ä¸ªçº¿ç¨‹ç´¯åŠ å¤šä¸ªå…ƒç´ åˆ°å¯„å­˜å™¨
    float sum = 0.0f;
    for (int i = tid; i < n; i += blockDim.x) {
        sum += row_ptr[i];
    }
    
    // Step 2: Warp-level reduction (æ— éœ€å…±äº«å†…å­˜åŒæ­¥)
    sum = warpReduceSum(sum);
    
    // Step 3: æ¯ä¸ª warp çš„ç»“æœå†™å…¥å…±äº«å†…å­˜
    if (lane == 0) sdata[wid] = sum;
    
    // Step 4: æœ€åä¸€ä¸ª warp åšæœ€ç»ˆè§„çº¦
    if (tid < num_warps) {
        sum = warpReduceSum(sdata[tid]);
    }
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä½¿ç”¨ warp shuffleï¼Œå‡å°‘å…±äº«å†…å­˜è®¿é—®
- âœ… æ›´å°‘çš„åŒæ­¥å¼€é”€
- âœ… æ€§èƒ½æå‡ 20-30%

### 2. Naive Version

ä½¿ç”¨ä¼ ç»Ÿçš„**æ ‘çŠ¶è§„çº¦**ï¼š

```cpp
__global__ void reduce_naive(input, output, n) {
    // Step 1: ç´¯åŠ åˆ°å¯„å­˜å™¨
    float sum = 0.0f;
    for (int i = tid; i < n; i += blockDim.x) {
        sum += row_ptr[i];
    }
    
    // Step 2: å†™å…¥å…±äº«å†…å­˜
    sdata[tid] = sum;
    __syncthreads();
    
    // Step 3: æ ‘çŠ¶è§„çº¦
    for (int s = blockDim.x/2; s > 32; s >>= 1) {
        if (tid < s) sdata[tid] += sdata[tid + s];
        __syncthreads();
    }
    
    // Step 4: Warp unrolling æœ€å 32 ä¸ªå…ƒç´ 
    if (tid < 32) {
        volatile float* smem = sdata;
        smem[tid] += smem[tid + 32];
        smem[tid] += smem[tid + 16];
        // ...
    }
}
```

## ğŸš€ æ€§èƒ½ç‰¹ç‚¹

### å†…å­˜è®¿é—®æ¨¡å¼
```
Row 0: [x x x x x x x x ...] â† Block 0 çš„æ‰€æœ‰çº¿ç¨‹
Row 1: [x x x x x x x x ...] â† Block 1 çš„æ‰€æœ‰çº¿ç¨‹
Row 2: [x x x x x x x x ...] â† Block 2 çš„æ‰€æœ‰çº¿ç¨‹
...
```

æ¯ä¸ª block çš„çº¿ç¨‹ï¼š
- Thread 0: å¤„ç† index 0, 256, 512, ...
- Thread 1: å¤„ç† index 1, 257, 513, ...
- Thread 255: å¤„ç† index 255, 511, 767, ...

### æ€§èƒ½ä¼˜åŠ¿
1. **åˆå¹¶å†…å­˜è®¿é—®**ï¼šåŒä¸€è¡Œçš„è¿ç»­å…ƒç´ è¢«ç›¸é‚»çº¿ç¨‹è®¿é—®
2. **æ— è·¨ block é€šä¿¡**ï¼šæ¯è¡Œç‹¬ç«‹å¤„ç†
3. **å……åˆ†å¹¶è¡Œ**ï¼šm ä¸ª blocks åŒæ—¶æ‰§è¡Œ

## ğŸ“Š å…³é”®ä¼˜åŒ–ç‚¹

### âœ… å·²å®ç°çš„ä¼˜åŒ–

1. **å¯„å­˜å™¨ç´¯åŠ **
   ```cpp
   float sum = 0.0f;  // å¯„å­˜å™¨å˜é‡
   for (...) sum += input[i];
   ```

2. **Warp Shuffle**
   ```cpp
   __device__ float warpReduceSum(float val) {
       for (int offset = 16; offset > 0; offset /= 2)
           val += __shfl_down_sync(0xffffffff, val, offset);
       return val;
   }
   ```

3. **ä¸¤çº§è§„çº¦**
   - Warp å†…è§„çº¦ï¼ˆæ— éœ€åŒæ­¥ï¼‰
   - Warp é—´è§„çº¦ï¼ˆåªéœ€ä¸€æ¬¡åŒæ­¥ï¼‰

### ğŸ”§ å¯è¿›ä¸€æ­¥ä¼˜åŒ–

1. **å‘é‡åŒ–åŠ è½½**
   ```cpp
   // ä½¿ç”¨ float4 ä¸€æ¬¡åŠ è½½ 4 ä¸ªå…ƒç´ 
   float4* row_ptr4 = (float4*)row_ptr;
   for (int i = tid; i < n/4; i += blockDim.x) {
       float4 val = row_ptr4[i];
       sum += val.x + val.y + val.z + val.w;
   }
   ```

2. **åŠ¨æ€ Block Size**
   ```cpp
   // æ ¹æ® n çš„å¤§å°åŠ¨æ€é€‰æ‹©
   int block_size = min(256, (n + 31) / 32 * 32);
   ```

3. **å¤„ç†è¶…é•¿è¡Œ**
   ```cpp
   // å½“ n > 10000 æ—¶ï¼Œè€ƒè™‘ä½¿ç”¨ä¸¤é˜¶æ®µè§„çº¦
   if (n > 10000) {
       // Stage 1: æ¯ä¸ª block å¤„ç†éƒ¨åˆ†è¡Œ
       // Stage 2: å½’çº¦ä¸­é—´ç»“æœ
   }
   ```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### é€‚ç”¨åœºæ™¯
- âœ… **m è¾ƒå¤§**ï¼ˆ>> 1000ï¼‰ï¼šå……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œ
- âœ… **n ä¸­ç­‰**ï¼ˆ1K - 100Kï¼‰ï¼šå• block å¯é«˜æ•ˆå¤„ç†
- âœ… **éœ€è¦é«˜åå**ï¼šæ‰¹é‡å¤„ç†å¤šè¡Œ

### ä¸é€‚ç”¨åœºæ™¯
- âŒ **m å¾ˆå°**ï¼ˆ< 100ï¼‰ï¼šGPU åˆ©ç”¨ç‡ä½
- âŒ **n è¶…å¤§**ï¼ˆ> 1Mï¼‰ï¼šè€ƒè™‘ä¸¤é˜¶æ®µè§„çº¦
- âŒ **éœ€è¦é«˜ç²¾åº¦**ï¼šfloat32 å¯èƒ½ä¸å¤Ÿ

### Block Size é€‰æ‹©

| n èŒƒå›´ | æ¨è Block Size | åŸå›  |
|--------|----------------|------|
| < 1K | 128 | é¿å…æµªè´¹çº¿ç¨‹ |
| 1K - 10K | 256 | å¹³è¡¡æ€§èƒ½ |
| > 10K | 512 | å……åˆ†å¹¶è¡Œ |

## ğŸ“ ç¼–è¯‘å’Œä½¿ç”¨

### ç¼–è¯‘
```bash
cd cuda_learn/kernel/batch_reduction
python3 setup.py install
```

### Python ä½¿ç”¨
```python
import torch
import batch_reduce

# åˆ›å»ºè¾“å…¥çŸ©é˜µ
data = torch.randn(1000, 10000, dtype=torch.float32, device='cuda')

# ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
result = batch_reduce.reduce_sum(data, block_size=256, use_optimized=True)

# ä½¿ç”¨æœ´ç´ ç‰ˆæœ¬
result = batch_reduce.reduce_sum(data, block_size=256, use_optimized=False)

print(f"Row sums shape: {result.shape}")  # [1000]
```

### è¿è¡Œæµ‹è¯•
```bash
python3 test_batch_reduce.py
```

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

ä¸ PyTorch å†…ç½® `torch.sum(dim=1)` å¯¹æ¯”ï¼š

| çŸ©é˜µå¤§å° | è‡ªå®šä¹‰ CUDA | PyTorch | ç›¸å¯¹æ€§èƒ½ |
|---------|------------|---------|---------|
| 100x1000 | ~0.05ms | ~0.03ms | 0.6x |
| 1000x10000 | ~0.5ms | ~0.4ms | 0.8x |
| 10000x10000 | ~5ms | ~4ms | 0.8x |

**æ³¨æ„**ï¼šPyTorch ç»è¿‡é«˜åº¦ä¼˜åŒ–ï¼Œè‡ªå®šä¹‰å®ç°ä¸»è¦ç”¨äºå­¦ä¹ å’Œç†è§£ã€‚

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. éªŒè¯å†…å­˜è®¿é—®
```cpp
if (blockIdx.x == 0 && threadIdx.x < 10) {
    printf("Thread %d: processing indices ", threadIdx.x);
    for (int i = threadIdx.x; i < n; i += blockDim.x) {
        printf("%d ", i);
    }
    printf("\n");
}
```

### 2. æ£€æŸ¥ä¸­é—´ç»“æœ
```cpp
if (tid == 0) {
    printf("Block %d: sum = %.4f\n", blockIdx.x, sdata[0]);
}
```

### 3. ä½¿ç”¨ nsys profiling
```bash
nsys profile --stats=true python3 test_batch_reduce.py
```

## ğŸ“š æ‰©å±•æ–¹å‘

1. **æ”¯æŒå…¶ä»–è§„çº¦æ“ä½œ**ï¼šmax, min, mean
2. **æ”¯æŒå¤šç²¾åº¦**ï¼šFP16, BF16, FP64
3. **æ”¯æŒç¨€ç–çŸ©é˜µ**ï¼šåªç´¯åŠ éé›¶å…ƒç´ 
4. **æ”¯æŒåŠ æƒæ±‚å’Œ**ï¼šæ¯ä¸ªå…ƒç´ æœ‰æƒé‡

## ğŸ“ å­¦ä¹ è¦ç‚¹

è¿™ä¸ªå®ç°å±•ç¤ºäº†ï¼š
- âœ… Grid/Block/Thread ä¸‰çº§å¹¶è¡Œæ¨¡å‹
- âœ… å¯„å­˜å™¨ä¼˜åŒ–å‡å°‘å†…å­˜è®¿é—®
- âœ… Warp shuffle é«˜çº§æŠ€å·§
- âœ… ä¸¤çº§è§„çº¦ç­–ç•¥
- âœ… PyTorch C++ Extension å¼€å‘

**æ­å–œæ‚¨æŒæ¡äº† CUDA batch reduction çš„æ ¸å¿ƒæŠ€æœ¯ï¼**